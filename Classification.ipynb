{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCOaOUwupThj"},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from transformers import BertTokenizerFast\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import BertTokenizerFast, BertForSequenceClassification\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yod3-alavHp8"},"outputs":[],"source":["emotion_to_label = {\n","    'admiration': 0, 'amusement': 1, 'anger': 2, 'annoyance': 3, 'approval': 4,\n","    'caring': 5, 'confusion': 6, 'curiosity': 7, 'desire': 8, 'disappointment': 9,\n","    'disapproval': 10, 'disgust': 11, 'embarrassment': 12, 'excitement': 13,\n","    'fear': 14, 'gratitude': 15, 'grief': 16, 'joy': 17, 'love': 18,\n","    'nervousness': 19, 'optimism': 20, 'pride': 21, 'realization': 22,\n","    'relief': 23, 'remorse': 24, 'sadness': 25, 'surprise': 26, 'neutral': 27\n","}\n","data = pd.read_csv('/content/drive/MyDrive/goemotions_1.csv')\n","emodata = data.drop(['id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear'], axis=1)\n","emodata['label'] = emodata.loc[:, emotion_to_label.keys()].idxmax(axis=1).map(emotion_to_label)\n","emodata = emodata[['text', 'label']]"]},{"cell_type":"code","source":["emodata = emodata.groupby('label').apply(lambda x: x.sample(n=300, replace=True) if len(x) > 150 else x)\n","label_counts = emodata['label'].value_counts()"],"metadata":{"id":"kEIfirggaWzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REYfOpajuCCf"},"outputs":[],"source":["tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(emotion_to_label))\n","bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(emotion_to_label))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3p9TaEB6ljni"},"outputs":[],"source":["def tokenize_data(texts, labels, tokenizer, max_length=128):\n","    tokenized = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n","    return tokenized, torch.tensor(labels)\n","\n","class EmotionDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['labels'] = self.labels[idx]\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","texts = emodata['text'].tolist()\n","labels = emodata['label'].tolist()\n","\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n","\n","\n","train_encodings, train_labels = tokenize_data(train_texts, train_labels, tokenizer)\n","val_encodings, val_labels = tokenize_data(val_texts, val_labels, tokenizer)\n","train_dataset = EmotionDataset(train_encodings, train_labels)\n","val_dataset = EmotionDataset(val_encodings, val_labels)\n","\n","\n","bert_train_encodings, bert_train_labels = tokenize_data(train_texts, train_labels, bert_tokenizer)\n","bert_val_encodings, bert_val_labels = tokenize_data(val_texts, val_labels, bert_tokenizer)\n","bert_train_dataset = EmotionDataset(bert_train_encodings, bert_train_labels)\n","bert_val_dataset = EmotionDataset(bert_val_encodings, bert_val_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7C-ANViuzeR"},"outputs":[],"source":["pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9qn2yMpuQfy"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from transformers import EvalPrediction\n","from transformers import Trainer, TrainingArguments\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions.argmax(-1)\n","    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=20,\n","    per_device_train_batch_size=128,\n","    per_device_eval_batch_size=128,\n","    logging_dir='./logs',\n","    logging_steps=10,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","bert_trainer = Trainer(\n","    model=bert_model,\n","    args=training_args,\n","    train_dataset=bert_train_dataset,\n","    eval_dataset=bert_val_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","\n","trainer.train()\n","results = trainer.evaluate()\n","print(\"RoBERTa Test Accuracy:\", results[\"eval_accuracy\"])\n","\n","bert_trainer.train()\n","bert_results = bert_trainer.evaluate()\n","print(\"BERT Test Accuracy:\", bert_results[\"eval_accuracy\"])"]}],"metadata":{"colab":{"provenance":[{"file_id":"1X9CqLyW-9gCl7ewFKm-MLMcgiyEhhcDy","timestamp":1713908989922},{"file_id":"1_EVZA2UeudP6yDE-qHjU7qfEuDVDXE4r","timestamp":1712084408362}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}